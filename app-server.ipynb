{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 21:06:49 [W] [service.go:132] login to server failed: i/o deadline reached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import the shared state and nodes modules\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory paths\n",
    "VECTOR_STORE_PATH = \"vector-db\"\n",
    "UPLOADED_FILES_DIR = \"uploaded_files\"\n",
    "HASH_FILE_PATH = \"uploaded_files/file_hashes.txt\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "os.makedirs(UPLOADED_FILES_DIR, exist_ok=True)\n",
    "\n",
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate MD5 hash of a file to identify duplicates\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def load_existing_hashes():\n",
    "    \"\"\"Load existing file hashes from storage\"\"\"\n",
    "    if not os.path.exists(HASH_FILE_PATH):\n",
    "        return {}\n",
    "    \n",
    "    file_hashes = {}\n",
    "    try:\n",
    "        with open(HASH_FILE_PATH, \"r\") as f:\n",
    "            for line in f:\n",
    "                if \":\" in line:\n",
    "                    hash_value, filename = line.strip().split(\":\", 1)\n",
    "                    file_hashes[hash_value] = filename\n",
    "    except Exception:\n",
    "        # If there's an error reading the file, start fresh\n",
    "        file_hashes = {}\n",
    "    \n",
    "    return file_hashes\n",
    "\n",
    "def save_file_hash(file_hash, filename):\n",
    "    \"\"\"Save a new file hash to storage\"\"\"\n",
    "    with open(HASH_FILE_PATH, \"a+\") as f:\n",
    "        f.write(f\"{file_hash}:{filename}\\n\")\n",
    "\n",
    "class ServerNodes:\n",
    "    \"\"\"Server-specific nodes for the StateGraph\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf_node(state):\n",
    "        \"\"\"Node to load and process PDF files\"\"\"\n",
    "        pdf_files = state.get(\"pdf_files\", [])\n",
    "        if not pdf_files:\n",
    "            return {\"status\": \"No PDF files provided\", **state}\n",
    "        \n",
    "        all_documents = []\n",
    "        # Load each PDF\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_file)\n",
    "                documents = loader.load()\n",
    "                all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"status\": f\"Error loading PDF {pdf_file}: {str(e)}\", \n",
    "                    # Add a placeholder response to satisfy the state requirements\n",
    "                    \"response\": f\"Error loading PDF: {str(e)}\",\n",
    "                    **state\n",
    "                }\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(all_documents)\n",
    "        \n",
    "        return {\n",
    "            \"status\": f\"Successfully loaded {len(pdf_files)} PDF(s) with {len(split_docs)} chunks\",\n",
    "            \"documents\": split_docs,\n",
    "            # Add a placeholder response to satisfy the state requirements\n",
    "            \"response\": f\"Successfully loaded {len(pdf_files)} PDF(s) with {len(split_docs)} chunks\",\n",
    "            **state\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_vector_store_node(state):\n",
    "        \"\"\"Node to update the vector store with new documents\"\"\"\n",
    "        documents = state.get(\"documents\", [])\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"status\": \"No documents to add to vector store\", \n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": \"No documents to add to vector store\",\n",
    "                **state\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Initialize embeddings\n",
    "            embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "            \n",
    "            # Check if vector store exists\n",
    "            if os.path.exists(VECTOR_STORE_PATH):\n",
    "                # Load existing vector store\n",
    "                vector_store = Chroma(persist_directory=VECTOR_STORE_PATH, embedding_function=embeddings)\n",
    "                # Add new documents\n",
    "                vector_store.add_documents(documents)\n",
    "            else:\n",
    "                # Create new vector store\n",
    "                vector_store = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=embeddings,\n",
    "                    persist_directory=VECTOR_STORE_PATH\n",
    "                )\n",
    "            \n",
    "            # Persist changes\n",
    "            vector_store.persist()\n",
    "            \n",
    "            success_message = f\"Successfully updated vector store with {len(documents)} documents\"\n",
    "            return {\n",
    "                \"status\": success_message,\n",
    "                \"vector_store\": vector_store,\n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": success_message,\n",
    "                **state\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error updating vector store: {str(e)}\"\n",
    "            return {\n",
    "                \"status\": error_message,\n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": error_message,\n",
    "                **state\n",
    "            }\n",
    "\n",
    "def build_server_stategraph():\n",
    "    \"\"\"Build the StateGraph for the server\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Add server-specific nodes\n",
    "    builder.add_node(\"load_pdf\", ServerNodes.load_pdf_node)\n",
    "    builder.add_node(\"update_vector_store\", ServerNodes.update_vector_store_node)\n",
    "    \n",
    "    # Add shared nodes (for chat functionality)\n",
    "    builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "    builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "    builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "    builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "    \n",
    "    # Define PDF processing flow\n",
    "    builder.add_conditional_edges(\n",
    "        \"user_input\",\n",
    "        lambda state: \"load_pdf\" if state.get(\"pdf_files\") else \"retrieve\"\n",
    "    )\n",
    "    builder.add_edge(\"load_pdf\", \"update_vector_store\")\n",
    "    builder.add_edge(\"update_vector_store\", END)\n",
    "    \n",
    "    # Define chat flow\n",
    "    builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "    builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "    builder.add_edge(\"update_memory\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    builder.set_entry_point(\"user_input\")\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Create a separate graph for PDF processing to avoid state validation issues\n",
    "def build_pdf_processing_graph():\n",
    "    \"\"\"Build a separate StateGraph just for PDF processing\"\"\"\n",
    "    # Create a custom state class for PDF processing that doesn't require the same fields\n",
    "    class PDFProcessState(dict):\n",
    "        pass\n",
    "    \n",
    "    builder = StateGraph(PDFProcessState)\n",
    "    \n",
    "    # Add PDF processing nodes\n",
    "    builder.add_node(\"load_pdf\", ServerNodes.load_pdf_node)\n",
    "    builder.add_node(\"update_vector_store\", ServerNodes.update_vector_store_node)\n",
    "    \n",
    "    # Define flow\n",
    "    builder.set_entry_point(\"load_pdf\")\n",
    "    builder.add_edge(\"load_pdf\", \"update_vector_store\")\n",
    "    builder.add_edge(\"update_vector_store\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Create the StateGraphs\n",
    "server_graph = build_server_stategraph()\n",
    "pdf_process_graph = build_pdf_processing_graph()\n",
    "\n",
    "def save_uploaded_files(files):\n",
    "    \"\"\"Save uploaded files to the designated directory and return their paths\"\"\"\n",
    "    saved_paths = []\n",
    "    duplicate_files = []\n",
    "    existing_hashes = load_existing_hashes()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        # Calculate file hash to check for duplicates\n",
    "        file_hash = get_file_hash(file.name)\n",
    "        \n",
    "        # Check if this file has been uploaded before\n",
    "        if file_hash in existing_hashes:\n",
    "            duplicate_files.append(os.path.basename(file.name))\n",
    "            continue\n",
    "        \n",
    "        # Create a unique filename with timestamp\n",
    "        original_filename = os.path.basename(file.name)\n",
    "        filename_without_ext, extension = os.path.splitext(original_filename)\n",
    "        new_filename = f\"{filename_without_ext}_{timestamp}_{i}{extension}\"\n",
    "        \n",
    "        # Define the save path\n",
    "        save_path = os.path.join(UPLOADED_FILES_DIR, new_filename)\n",
    "        \n",
    "        # Copy the file to the uploads directory\n",
    "        shutil.copy2(file.name, save_path)\n",
    "        saved_paths.append(save_path)\n",
    "        \n",
    "        # Save the hash to prevent future duplicates\n",
    "        save_file_hash(file_hash, new_filename)\n",
    "    \n",
    "    return saved_paths, duplicate_files\n",
    "\n",
    "def list_uploaded_files():\n",
    "    \"\"\"List all previously uploaded PDF files with metadata\"\"\"\n",
    "    if not os.path.exists(UPLOADED_FILES_DIR):\n",
    "        return {\"data\": [], \"headers\": [\"Filename\", \"Size (KB)\", \"Upload Date\"]}\n",
    "    \n",
    "    files_data = []\n",
    "    for filename in os.listdir(UPLOADED_FILES_DIR):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            file_path = os.path.join(UPLOADED_FILES_DIR, filename)\n",
    "            # Get file stats\n",
    "            file_stats = os.stat(file_path)\n",
    "            size_kb = file_stats.st_size / 1024\n",
    "            upload_date = datetime.fromtimestamp(file_stats.st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            \n",
    "            # Add as a list (row) instead of a dictionary\n",
    "            files_data.append([filename, f\"{size_kb:.1f}\", upload_date])\n",
    "    \n",
    "    # Sort by upload date (newest first)\n",
    "    files_data.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return {\"data\": files_data, \"headers\": [\"Filename\", \"Size (KB)\", \"Upload Date\"]}\n",
    "\n",
    "def process_pdfs(pdf_files):\n",
    "    \"\"\"Process uploaded PDF files and update the vector store\"\"\"\n",
    "    if not pdf_files:\n",
    "        return \"No PDF files uploaded\"\n",
    "    \n",
    "    # Save the uploaded files to the upload directory, checking for duplicates\n",
    "    saved_paths, duplicate_files = save_uploaded_files(pdf_files)\n",
    "    \n",
    "    # Handle case where all files are duplicates\n",
    "    if not saved_paths and duplicate_files:\n",
    "        return f\"All files are duplicates and were not uploaded again: {', '.join(duplicate_files)}\"\n",
    "    \n",
    "    # Handle case with some duplicates\n",
    "    duplicate_message = \"\"\n",
    "    if duplicate_files:\n",
    "        duplicate_message = f\"\\nSkipped duplicate files: {', '.join(duplicate_files)}\"\n",
    "    \n",
    "    # If no new files after duplicate check\n",
    "    if not saved_paths:\n",
    "        return f\"No new files to process.{duplicate_message}\"\n",
    "    \n",
    "    try:\n",
    "        # Use the dedicated PDF processing graph instead of the main server graph\n",
    "        final_state = pdf_process_graph.invoke({\n",
    "            \"pdf_files\": saved_paths\n",
    "        })\n",
    "        \n",
    "        status = final_state.get(\"status\", \"Processing completed\")\n",
    "        return f\"{status}\\nFiles saved to {UPLOADED_FILES_DIR} directory.{duplicate_message}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDFs: {str(e)}\\nFiles were saved to {UPLOADED_FILES_DIR} directory.{duplicate_message}\"\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"Handle chat interaction\"\"\"\n",
    "    # Run the graph with user query\n",
    "    final_state = server_graph.invoke({\n",
    "        \"user_query\": message\n",
    "    })\n",
    "    \n",
    "    # Extract the AI's response\n",
    "    ai_response = final_state.get(\"response\", \"I don't have an answer for that.\")\n",
    "    \n",
    "    return ai_response\n",
    "\n",
    "# Create Gradio interface with tabs for different functions\n",
    "with gr.Blocks(title=\"IZTECH Telecom RAG Server ðŸ¤–\", theme=gr.themes.Soft()) as demo:   \n",
    "    with gr.Tabs():\n",
    "        # PDF Upload Tab\n",
    "        with gr.TabItem(\"PDF Upload\"):\n",
    "            \n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"Upload PDF files to update the vector database with new documents. Duplicate files will be skipped.\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                pdf_input = gr.File(\n",
    "                    file_count=\"multiple\", \n",
    "                    file_types=['.pdf'], \n",
    "                    label=\"Upload PDF Files\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                process_btn = gr.Button(\"ADD to Vector DB\")\n",
    "                status_output = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
    "            \n",
    "            with gr.Row(elem_classes=\"center-row\"):\n",
    "                files_list = gr.DataFrame(\n",
    "                    headers=[\"Filename\", \"Size (KB)\", \"Upload Date\"],\n",
    "                    datatype=[\"str\", \"str\", \"str\"],\n",
    "                    label=\"Uploaded PDFs\",\n",
    "                    interactive=False\n",
    "                )\n",
    "\n",
    "            # Connect process button to process_pdfs function\n",
    "            process_btn.click(\n",
    "                fn=process_pdfs,\n",
    "                inputs=[pdf_input],\n",
    "                outputs=[status_output]\n",
    "            ).then(\n",
    "                fn=list_uploaded_files,\n",
    "                outputs=[files_list]\n",
    "            )\n",
    "\n",
    "            # Initialize the files list on page load\n",
    "            demo.load(\n",
    "                fn=list_uploaded_files,\n",
    "                outputs=[files_list]\n",
    "            )\n",
    "        \n",
    "        # Chat Tab\n",
    "        with gr.TabItem(\"Chat\"):\n",
    "            chatbot = gr.ChatInterface(\n",
    "                fn=respond,\n",
    "                title=\"IZTECH-Server Telecom RAG Assistant ðŸ¤–\",\n",
    "            )\n",
    "\n",
    "# Launch the application\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
