{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes\n",
    "\n",
    "def build_stategraph():\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Adding nodes\n",
    "    builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "    builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "    builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "    builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "    \n",
    "    # Defining flow\n",
    "    builder.set_entry_point(\"user_input\")\n",
    "    builder.add_edge(\"user_input\", \"retrieve\")\n",
    "    builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "    builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "    builder.add_edge(\"update_memory\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "def main():\n",
    "    rag_graph = build_stategraph()\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your query: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        \n",
    "        for step in rag_graph.stream({\"user_query\": query}):\n",
    "            print(\"Response:\", step)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes\n",
    "\n",
    "def build_stategraph():\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Adding nodes\n",
    "    builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "    builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "    builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "    builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "    \n",
    "    # Defining flow\n",
    "    builder.set_entry_point(\"user_input\")\n",
    "    builder.add_edge(\"user_input\", \"retrieve\")\n",
    "    builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "    builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "    builder.add_edge(\"update_memory\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Build the stategraph only once\n",
    "rag_graph = build_stategraph()\n",
    "\n",
    "def process_query1(query):\n",
    "    # Collect responses from the stategraph and join them in a single string.\n",
    "    responses = []\n",
    "    for step in rag_graph.stream({\"user_query\": query}):\n",
    "        responses.append(str(step))\n",
    "    return \"\\n\".join(responses)\n",
    "\n",
    "def process_query(query):\n",
    "    # Collect responses from the stategraph\n",
    "    last_response = None\n",
    "    for step in rag_graph.stream({\"user_query\": query}):\n",
    "        last_response = step  # Keep updating to get the last response\n",
    "    \n",
    "    # Convert the last response to string if it's not already\n",
    "    return str(last_response) if last_response else \"No response generated.\"\n",
    "\n",
    "\n",
    "# Create the Gradio interface.\n",
    "iface = gr.Interface(\n",
    "    fn=process_query,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your query here...\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"RAG Graph Interface\",\n",
    "    description=\"Enter a query and get a response from the stategraph.\"\n",
    ")\n",
    "\n",
    "# Launch Gradio; for notebooks, share=False usually suffices.\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 20:10:58 [W] [service.go:132] login to server failed: i/o deadline reached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes\n",
    "\n",
    "def build_stategraph():\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Adding nodes\n",
    "    builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "    builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "    builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "    builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "    \n",
    "    # Defining flow\n",
    "    builder.set_entry_point(\"user_input\")\n",
    "    builder.add_edge(\"user_input\", \"retrieve\")\n",
    "    builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "    builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "    builder.add_edge(\"update_memory\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Create the StateGraph\n",
    "rag_graph = build_stategraph()\n",
    "\n",
    "def respond(message, history):\n",
    "    # Run the graph and get the final state\n",
    "    final_state = rag_graph.invoke({\"user_query\": message})\n",
    "    \n",
    "    # Extract the AI's response from the final state\n",
    "    # Adjust this based on your AgentState structure\n",
    "    ai_response = final_state.get(\"response\", \"I don't have an answer for that.\")\n",
    "    \n",
    "    return ai_response\n",
    "\n",
    "# Create and launch the Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    title=\"LangGraph RAG Assistant\",\n",
    "    description=\"Ask me anything and I'll use a retrieval-augmented generation system to answer.\",\n",
    "    examples=[\"Tell me about machine learning\", \"What is LangGraph?\", \"How does RAG work?\"],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "Running on public URL: https://f37c2011bc38fbcc79.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f37c2011bc38fbcc79.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import warnings\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import the shared state and nodes modules\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory paths\n",
    "VECTOR_STORE_PATH = \"vector-db\"\n",
    "UPLOADED_FILES_DIR = \"uploaded_files\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(VECTOR_STORE_PATH, exist_ok=True)\n",
    "os.makedirs(UPLOADED_FILES_DIR, exist_ok=True)\n",
    "\n",
    "class ServerNodes:\n",
    "    \"\"\"Server-specific nodes for the StateGraph\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pdf_node(state):\n",
    "        \"\"\"Node to load and process PDF files\"\"\"\n",
    "        pdf_files = state.get(\"pdf_files\", [])\n",
    "        if not pdf_files:\n",
    "            return {\"status\": \"No PDF files provided\", **state}\n",
    "        \n",
    "        all_documents = []\n",
    "        # Load each PDF\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                loader = PyPDFLoader(pdf_file)\n",
    "                documents = loader.load()\n",
    "                all_documents.extend(documents)\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"status\": f\"Error loading PDF {pdf_file}: {str(e)}\", \n",
    "                    # Add a placeholder response to satisfy the state requirements\n",
    "                    \"response\": f\"Error loading PDF: {str(e)}\",\n",
    "                    **state\n",
    "                }\n",
    "        \n",
    "        # Split documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        split_docs = text_splitter.split_documents(all_documents)\n",
    "        \n",
    "        return {\n",
    "            \"status\": f\"Successfully loaded {len(pdf_files)} PDF(s) with {len(split_docs)} chunks\",\n",
    "            \"documents\": split_docs,\n",
    "            # Add a placeholder response to satisfy the state requirements\n",
    "            \"response\": f\"Successfully loaded {len(pdf_files)} PDF(s) with {len(split_docs)} chunks\",\n",
    "            **state\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def update_vector_store_node(state):\n",
    "        \"\"\"Node to update the vector store with new documents\"\"\"\n",
    "        documents = state.get(\"documents\", [])\n",
    "        if not documents:\n",
    "            return {\n",
    "                \"status\": \"No documents to add to vector store\", \n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": \"No documents to add to vector store\",\n",
    "                **state\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Initialize embeddings\n",
    "            embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") \n",
    "            \n",
    "            # Check if vector store exists\n",
    "            if os.path.exists(VECTOR_STORE_PATH):\n",
    "                # Load existing vector store\n",
    "                vector_store = Chroma(persist_directory=VECTOR_STORE_PATH, embedding_function=embeddings)\n",
    "                # Add new documents\n",
    "                vector_store.add_documents(documents)\n",
    "            else:\n",
    "                # Create new vector store\n",
    "                vector_store = Chroma.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=embeddings,\n",
    "                    persist_directory=VECTOR_STORE_PATH\n",
    "                )\n",
    "            \n",
    "            # Persist changes\n",
    "            vector_store.persist()\n",
    "            \n",
    "            success_message = f\"Successfully updated vector store with {len(documents)} documents\"\n",
    "            return {\n",
    "                \"status\": success_message,\n",
    "                \"vector_store\": vector_store,\n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": success_message,\n",
    "                **state\n",
    "            }\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error updating vector store: {str(e)}\"\n",
    "            return {\n",
    "                \"status\": error_message,\n",
    "                # Add a placeholder response to satisfy the state requirements\n",
    "                \"response\": error_message,\n",
    "                **state\n",
    "            }\n",
    "\n",
    "def build_server_stategraph():\n",
    "    \"\"\"Build the StateGraph for the server\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "    \n",
    "    # Add server-specific nodes\n",
    "    builder.add_node(\"load_pdf\", ServerNodes.load_pdf_node)\n",
    "    builder.add_node(\"update_vector_store\", ServerNodes.update_vector_store_node)\n",
    "    \n",
    "    # Add shared nodes (for chat functionality)\n",
    "    builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "    builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "    builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "    builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "    \n",
    "    # Define PDF processing flow\n",
    "    builder.add_conditional_edges(\n",
    "        \"user_input\",\n",
    "        lambda state: \"load_pdf\" if state.get(\"pdf_files\") else \"retrieve\"\n",
    "    )\n",
    "    builder.add_edge(\"load_pdf\", \"update_vector_store\")\n",
    "    builder.add_edge(\"update_vector_store\", END)\n",
    "    \n",
    "    # Define chat flow\n",
    "    builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "    builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "    builder.add_edge(\"update_memory\", END)\n",
    "    \n",
    "    # Set entry point\n",
    "    builder.set_entry_point(\"user_input\")\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Create a separate graph for PDF processing to avoid state validation issues\n",
    "def build_pdf_processing_graph():\n",
    "    \"\"\"Build a separate StateGraph just for PDF processing\"\"\"\n",
    "    # Create a custom state class for PDF processing that doesn't require the same fields\n",
    "    class PDFProcessState(dict):\n",
    "        pass\n",
    "    \n",
    "    builder = StateGraph(PDFProcessState)\n",
    "    \n",
    "    # Add PDF processing nodes\n",
    "    builder.add_node(\"load_pdf\", ServerNodes.load_pdf_node)\n",
    "    builder.add_node(\"update_vector_store\", ServerNodes.update_vector_store_node)\n",
    "    \n",
    "    # Define flow\n",
    "    builder.set_entry_point(\"load_pdf\")\n",
    "    builder.add_edge(\"load_pdf\", \"update_vector_store\")\n",
    "    builder.add_edge(\"update_vector_store\", END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "# Create the StateGraphs\n",
    "server_graph = build_server_stategraph()\n",
    "pdf_process_graph = build_pdf_processing_graph()\n",
    "\n",
    "def save_uploaded_files(files):\n",
    "    \"\"\"Save uploaded files to the designated directory and return their paths\"\"\"\n",
    "    saved_paths = []\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        # Create a unique filename with timestamp\n",
    "        original_filename = os.path.basename(file.name)\n",
    "        filename_without_ext, extension = os.path.splitext(original_filename)\n",
    "        new_filename = f\"{filename_without_ext}_{timestamp}_{i}{extension}\"\n",
    "        \n",
    "        # Define the save path\n",
    "        save_path = os.path.join(UPLOADED_FILES_DIR, new_filename)\n",
    "        \n",
    "        # Copy the file to the uploads directory\n",
    "        shutil.copy2(file.name, save_path)\n",
    "        saved_paths.append(save_path)\n",
    "    \n",
    "    return saved_paths\n",
    "\n",
    "def list_uploaded_files():\n",
    "    \"\"\"List all previously uploaded PDF files\"\"\"\n",
    "    if not os.path.exists(UPLOADED_FILES_DIR):\n",
    "        return []\n",
    "    \n",
    "    files = [f for f in os.listdir(UPLOADED_FILES_DIR) if f.lower().endswith('.pdf')]\n",
    "    return sorted(files, reverse=True)  # Sort by newest first\n",
    "\n",
    "def process_pdfs(pdf_files):\n",
    "    \"\"\"Process uploaded PDF files and update the vector store\"\"\"\n",
    "    if not pdf_files:\n",
    "        return \"No PDF files uploaded\"\n",
    "    \n",
    "    # Save the uploaded files to the upload directory\n",
    "    saved_paths = save_uploaded_files(pdf_files)\n",
    "    \n",
    "    try:\n",
    "        # Use the dedicated PDF processing graph instead of the main server graph\n",
    "        final_state = pdf_process_graph.invoke({\n",
    "            \"pdf_files\": saved_paths\n",
    "        })\n",
    "        \n",
    "        status = final_state.get(\"status\", \"Processing completed\")\n",
    "        return f\"{status}\\nFiles saved to {UPLOADED_FILES_DIR} directory.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error processing PDFs: {str(e)}\\nFiles were saved to {UPLOADED_FILES_DIR} directory.\"\n",
    "\n",
    "def respond(message, history):\n",
    "    \"\"\"Handle chat interaction\"\"\"\n",
    "    # Run the graph with user query\n",
    "    final_state = server_graph.invoke({\n",
    "        \"user_query\": message\n",
    "    })\n",
    "    \n",
    "    # Extract the AI's response\n",
    "    ai_response = final_state.get(\"response\", \"I don't have an answer for that.\")\n",
    "    \n",
    "    return ai_response\n",
    "\n",
    "# Create Gradio interface with tabs for different functions\n",
    "with gr.Blocks(title=\"IZTECH Telecom RAG Server 🤖\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# IZTECH Telecom RAG Server\")\n",
    "    \n",
    "    with gr.Tabs():\n",
    "        # PDF Upload Tab\n",
    "        with gr.TabItem(\"PDF Upload\"):\n",
    "            with gr.Row():\n",
    "                pdf_input = gr.File(\n",
    "                    file_count=\"multiple\", \n",
    "                    file_types=['.pdf'], \n",
    "                    label=\"Upload PDF Files\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                process_btn = gr.Button(\"Process PDFs\")\n",
    "                status_output = gr.Textbox(label=\"Processing Status\", interactive=False)\n",
    "            \n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"### Previously Uploaded Files\")\n",
    "                files_list = gr.Dataframe(\n",
    "                    headers=[\"Filename\"],\n",
    "                    datatype=[\"str\"],\n",
    "                    label=\"Uploaded PDFs\",\n",
    "                    interactive=False\n",
    "                )\n",
    "            \n",
    "            # Connect process button to process_pdfs function\n",
    "            process_btn.click(\n",
    "                fn=process_pdfs,\n",
    "                inputs=[pdf_input],\n",
    "                outputs=[status_output]\n",
    "            ).then(\n",
    "                fn=lambda: {\"Filename\": list_uploaded_files()},\n",
    "                outputs=[files_list]\n",
    "            )\n",
    "            \n",
    "            # Initialize the files list on page load\n",
    "            demo.load(\n",
    "                fn=lambda: {\"Filename\": list_uploaded_files()},\n",
    "                outputs=[files_list]\n",
    "            )\n",
    "        \n",
    "        # Chat Tab\n",
    "        with gr.TabItem(\"Chat\"):\n",
    "            chatbot = gr.ChatInterface(\n",
    "                fn=respond,\n",
    "                title=\"Chat with RAG System\",\n",
    "                description=\"Ask questions about the uploaded PDFs\"\n",
    "            )\n",
    "\n",
    "# Launch the application\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
