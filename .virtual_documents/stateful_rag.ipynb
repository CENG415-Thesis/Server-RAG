from langgraph.graph import StateGraph, END
from typing import TypedDict, List
from langchain.memory import ConversationBufferMemory
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_community.llms import Ollama

# AgentState structure
class AgentState(TypedDict):
    user_query: str
    retrieved_docs: List[str]
    response: str
    chat_history: str

# Hafıza ve vektör veritabanı yüklemeliyim Unutma !
DB_NAME = "vector-db"
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)
retriever = vectorstore.as_retriever()
llm = Ollama(model="llama3.2")
memory = ConversationBufferMemory()

# (Nodes)
def user_input_node(state: AgentState):
    #query alır
    return {"user_query": state["user_query"]}

def retrieve_node(state: AgentState):
    #vector dbden alakalıları çekterim
    retrieved_docs = vectorstore.similarity_search(state["user_query"], k=10)
    return {"retrieved_docs": [doc.page_content for doc in retrieved_docs]}

def generate_response_node(state: AgentState):
    #cevabı generate ettiririm
    combined_context = "\n".join(state["retrieved_docs"])
    prompt = f"""
    You are a telecom assistant. Your answers should be based on the retrieved context below.
    
    Context: {combined_context}
    
    Question: {state['user_query']}
    
    Answer:
    """
    response = llm.invoke(prompt)
    return {"response": response}

def update_memory_node(state: AgentState):
    #konuşma geçmişini güncellerim
    memory.save_context({"input": state["user_query"]}, {"output": state["response"]})
    return {"chat_history": memory.load_memory_variables({}).get("history", "")}

# StateGraph 
builder = StateGraph(AgentState)
builder.add_node("user_input", user_input_node)
builder.add_node("retrieve", retrieve_node)
builder.add_node("generate_response", generate_response_node)
builder.add_node("update_memory", update_memory_node)


builder.set_entry_point("user_input")
builder.add_edge("user_input", "retrieve")
builder.add_edge("retrieve", "generate_response")
builder.add_edge("generate_response", "update_memory")
builder.add_edge("update_memory", END)


rag_graph = builder.compile()




# Örnek ççalıştırmak için
test_input = {"user_query": "What is 5G Network slicing?"}
for s in rag_graph.stream(test_input):
    print(s)





from langgraph.graph import StateGraph, END
from typing import TypedDict, List
from langchain.memory import ConversationBufferMemory
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.llms import Ollama

# Grader için Prompt
GRADER_PROMPT_TEMPLATE = """
You are an expert evaluator. Grade the response based on the following criteria:
1. Groundedness: Does the response fully address the question using the provided context?
2. Answer Relevance: Is the response relevant and directly answering the question?
3. Context Relevance: Is the response actually correct based on the context?

Context:
{context}

Question:
{question}

Response:
{response}

---

Provide a float score from 0 to 1 for each criteria and a short explanation for the score.
"""

class Grader:
    def __init__(self):
        self.model = Ollama(model="llama3.2")  # Grading LLM

    def grade(self, context: str, question: str, response: str) -> str:
        prompt_template = ChatPromptTemplate.from_template(GRADER_PROMPT_TEMPLATE)
        prompt = prompt_template.format(
            context=context, question=question, response=response
        )
        return self.model.invoke(prompt)

# AgentState yapısı
class AgentState(TypedDict):
    user_query: str
    retrieved_docs: List[str]
    response: str
    chat_history: str
    evaluation_score: float

# Hafıza ve vektör veritabanı yükleme
DB_NAME = "vector-db"
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)
retriever = vectorstore.as_retriever()
llm = Ollama(model="llama3.2")
memory = ConversationBufferMemory()
grader = Grader()

# Düğümler (Nodes)
def user_input_node(state: AgentState):
    return {"user_query": state["user_query"]}

def retrieve_node(state: AgentState):
    retrieved_docs = vectorstore.similarity_search(state["user_query"], k=10)
    return {"retrieved_docs": [doc.page_content for doc in retrieved_docs]}

def generate_response_node(state: AgentState):
    combined_context = "\n".join(state["retrieved_docs"])
    prompt = f"""
    You are a telecom assistant. Your answers should be based on the retrieved context below.
    
    Context: {combined_context}
    
    Question: {state['user_query']}
    
    Answer:
    """
    response = llm.invoke(prompt)
    return {"response": response}

def evaluate_response_node(state: AgentState):
    evaluation_result = grader.grade(
        context="\n".join(state["retrieved_docs"]),
        question=state["user_query"],
        response=state["response"]
    )
    
    print("Grader Output:", evaluation_result)  # Debug için çıktıyı yazdır
    
    score_values = []
    score_lines = evaluation_result.split("\n")
    
    for line in score_lines:
        if any(keyword in line for keyword in ["Overall Score", "Final Score", "Total Score"]):
            try:
                score = float(line.split(":")[1].strip().replace('*', ''))
                score_values.append(score)
            except (IndexError, ValueError):
                continue
    
    final_score = max(score_values, default=0.0)  # Eğer hiç uygun skor bulunmazsa 0.0 ata
    return {"evaluation_score": final_score}

def revise_response_node(state: AgentState):
    if state["evaluation_score"] < 0.5:
        return {"response": generate_response_node(state)["response"]}
    return {}

def update_memory_node(state: AgentState):
    memory.save_context({"input": state["user_query"]}, {"output": state["response"]})
    return {"chat_history": memory.load_memory_variables({}).get("history", "")}

# StateGraph oluşturma
builder = StateGraph(AgentState)
builder.add_node("user_input", user_input_node)
builder.add_node("retrieve", retrieve_node)
builder.add_node("generate_response", generate_response_node)
builder.add_node("evaluate_response", evaluate_response_node)
builder.add_node("revise_response", revise_response_node)
builder.add_node("update_memory", update_memory_node)

# İş akışını belirleme
builder.set_entry_point("user_input")
builder.add_edge("user_input", "retrieve")
builder.add_edge("retrieve", "generate_response")
builder.add_edge("generate_response", "evaluate_response")
builder.add_conditional_edges(
    "evaluate_response", 
    lambda state: "revise_response" if state["evaluation_score"] < 0.5 else "update_memory",
    {"revise_response": "revise_response", "update_memory": "update_memory"}
)
builder.add_edge("revise_response", "generate_response")
builder.add_edge("update_memory", END)

# Graph oluştur
rag_graph = builder.compile()

# Örnek çalıştırma
test_input = {"user_query": "What is 5G Network slicing?"}
for s in rag_graph.stream(test_input):
    print(s)



# Örnek çalıştırma
test_input = {"user_query": "What is 5G Network slicing?"}
for s in rag_graph.stream(test_input):
    print(s)



