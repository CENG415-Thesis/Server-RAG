{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f97972-9bc0-42be-a94b-b35b5ec661a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes  # Import the Nodes class\n",
    "\n",
    "# StateGraph \n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "\n",
    "builder.set_entry_point(\"user_input\")\n",
    "builder.add_edge(\"user_input\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "builder.add_edge(\"update_memory\", END)\n",
    "\n",
    "rag_graph = builder.compile()\n",
    "\n",
    "\n",
    "#Client UI\n",
    "\n",
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Mevcut vektör veritabanının adı\n",
    "DB_NAME = \"vector-db\"\n",
    "\n",
    "# Vektör veritabanını yükleme\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Ollama modelini başlat\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Chat Memory (Konuşma geçmişi için)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "def query_rag_pipeline(user_query, history):\n",
    "    # Konuşma geçmişini al\n",
    "    chat_history = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    # RAG ile en alakalı belgeleri al\n",
    "    retrieved_docs = vectorstore.similarity_search(user_query, k=10)\n",
    "    combined_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Prompt formatı\n",
    "    prompt = f\"\"\"\n",
    "    You are a telecom assistant. Your answers should be based on the context and chat history provided. If the context is not relevant to the user's query, politely state that you do not have the required information.\n",
    "\n",
    "    Chat History: {chat_history}\n",
    "    \n",
    "    Context: {combined_context}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Konuşma geçmişini güncelle\n",
    "    memory.save_context({\"input\": user_query}, {\"output\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "gr.ChatInterface(fn=query_rag_pipeline, title=\"Chatbot RAG Assistant 🤖\", type=\"messages\").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bbbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 18:02:09 [W] [service.go:132] login to server failed: i/o deadline reached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\blocks.py\", line 1518, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\utils.py\", line 793, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\chat_interface.py\", line 623, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\CAN\\AppData\\Local\\Temp\\ipykernel_23984\\1214548472.py\", line 38, in query_rag_pipeline\n",
      "    rag_graph.run(state)\n",
      "AttributeError: 'CompiledStateGraph' object has no attribute 'run'\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes  # Import the Nodes class\n",
    "from helpers.config import embeddings, vectorstore, retriever, llm, memory  # Import the configuration\n",
    "\n",
    "# StateGraph \n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "\n",
    "builder.set_entry_point(\"user_input\")\n",
    "builder.add_edge(\"user_input\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "builder.add_edge(\"update_memory\", \"user_input\")  # Loop back to user_input instead of END\n",
    "\n",
    "rag_graph = builder.compile()\n",
    "#Client UI\n",
    "\n",
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Mevcut vektör veritabanının adı\n",
    "DB_NAME = \"vector-db\"\n",
    "\n",
    "\n",
    "def query_rag_pipeline(user_query, history):\n",
    "    # Initialize the state\n",
    "    state = AgentState()\n",
    "    state[\"user_query\"] = user_query\n",
    "    \n",
    "    # Process the query through the StateGraph\n",
    "    rag_graph.run(state)\n",
    "    \n",
    "    # Get the response from the state\n",
    "    response = state.get(\"response\", \"Sorry, I couldn't generate a response.\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "gr.ChatInterface(fn=query_rag_pipeline, title=\"Chatbot RAG Assistant 🤖\", type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1ce333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 18:07:11 [W] [service.go:132] login to server failed: i/o deadline reached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\blocks.py\", line 1518, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\utils.py\", line 793, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\gradio\\chat_interface.py\", line 623, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2364, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 864, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\CAN\\AppData\\Local\\Temp\\ipykernel_23984\\3190189769.py\", line 30, in query_rag_pipeline\n",
      "    for _ in rag_graph.stream(state):\n",
      "  File \"c:\\Users\\CAN\\anaconda3\\envs\\LangChainEnv\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1690, in stream\n",
      "    raise GraphRecursionError(msg)\n",
      "langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from modules.agent_state import AgentState\n",
    "from modules.nodes import Nodes  # Import the Nodes class\n",
    "from helpers.config import embeddings, vectorstore, retriever, llm, memory  # Import the configuration\n",
    "\n",
    "# StateGraph \n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"user_input\", Nodes.user_input_node)\n",
    "builder.add_node(\"retrieve\", Nodes.retrieve_node)\n",
    "builder.add_node(\"generate_response\", Nodes.generate_response_node)\n",
    "builder.add_node(\"update_memory\", Nodes.update_memory_node)\n",
    "\n",
    "builder.set_entry_point(\"user_input\")\n",
    "builder.add_edge(\"user_input\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "builder.add_edge(\"generate_response\", \"update_memory\")\n",
    "builder.add_edge(\"update_memory\", \"user_input\")  # Loop back to user_input instead of END\n",
    "\n",
    "rag_graph = builder.compile()\n",
    "\n",
    "# Client UI\n",
    "\n",
    "import gradio as gr\n",
    "def query_rag_pipeline(user_query, history):\n",
    "    # Initialize the state\n",
    "    state = AgentState()\n",
    "    state[\"user_query\"] = user_query\n",
    "    \n",
    "    # Process the query through the StateGraph\n",
    "    for _ in rag_graph.stream(state):\n",
    "        pass\n",
    "    \n",
    "    # Get the response from the state\n",
    "    response = state.get(\"response\", \"Sorry, I couldn't generate a response.\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "\n",
    "gr.ChatInterface(fn=query_rag_pipeline, title=\"Chatbot RAG Assistant 🤖\", type=\"messages\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f166d0a-b8e4-49b7-87ce-d37bc3813fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': {'user_query': 'What is 5G Network slicing?'}}\n",
      "{'retrieve': {'retrieved_docs': ['slicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling\\nremote hands-on computer networking education: the nitos testbed approach,” in 2023 IEEE Integrated\\nSTEM Education Conference (ISEC).\\nIEEE, 2023, pp. 132–138.\\n[513] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ue statistics time-series (cqi) in lte networks,”\\n2022.\\n[514] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ue network traffic time-series (applications,\\nthroughput, latency, cqi) in lte/5g networks,” IEEE Dataport, 2022.\\n[515] D. Spathis and F. Kawsar, “The first step is the hardest: Pitfalls of representing and tokenizing temporal\\ndata for large language models,” Journal of the American Medical Informatics Association, vol. 31, no. 9,\\npp. 2151–2158, 2024.', 'και\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.\\n[509] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ml-based traffic steering for heterogeneous ultra-\\ndense beyond-5g networks,” in 2023 IEEE Wireless Communications and Networking Conference (WCNC).\\nIEEE, 2023, pp. 1–6.\\n[510] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ai-driven service-aware real-time slicing for\\nbeyond 5g networks,” in IEEE INFOCOM 2022-IEEE Conference on Computer Communications Work-\\n234\\nshops (INFOCOM WKSHPS).\\nIEEE, 2022, pp. 1–6.\\n[511] T. Tsourdinis, I. Chatzistefanidis, N. Makris, T. Korakis, N. Nikaein, and S. Fdida, “Service-aware real-time\\nslicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling', 'Additionally, it covers 3GPP standards for mobile networks such as 4G LTE and 5G, which are critical for modern\\nmobile communication. ETSI standards from the European Telecommunications Standards Institute for mobile\\nand broadband networks, and IETF standards for internet protocols like TCP/IP, are also within its scope. Further-\\nmore, StandardGPT can address 5G and next-generation network standards, along with network security standards\\n(e.g., ISO/IEC 27033) and compliance with national regulatory frameworks like those from the FCC in the U.S.\\nThis broad coverage makes StandardGPT a useful tool for navigating both technical and regulatory aspects of the\\ntelecommunications industry.\\nOpenAI also proposed a more specific model for telecommunication: TelecomGPT [491]. TelecomGPT inludes\\ntelecommunication knowledge but it is also capable of doing tasks such as VoIP and analyzing SIP logs, help', 'into the building-block approach for AI factories.\\n7.5\\nConvergence of AI and RAN\\nThe advent and adoption of 5G technology has introduced a new level of speed and capability in networks, en-\\nabling them to deliver not only voice, but also Internet-based services, data, and streaming. This generalization of\\ntelecom networks into a data service network allows the introduction of LLM-driven services. A further benefit\\ncomes from the flexibility built into 5G network architecture, allowing different components to be strategically\\ndistributed in different locations, from the radio unit (RU) site at the very edge, to DU sites (such as MSOs), to\\nthe CU which has more space and power available. This flexibility also means that components can be decoupled,\\nsuch that following Open RAN (O-RAN) specifications enables a merger of the best solutions for both RAN and\\nAI processing while meeting latency demands [247]. This flexibility allows smaller footprints for AI inference to', 'policy generation, making them a practical solution for real-world use cases.\\nSLMs are particularly well-suited for low-latency and resource-constrained environments, such as those found at\\nthe network edge. As edge computing becomes a cornerstone of 6G networks, the ability to deploy intelligent\\nmodels close to the end-user is essential for minimizing latency and improving service responsiveness. SLMs can\\nefficiently operate in these decentralized settings, enabling localized decision-making and real-time adjustments\\nwithout relying on centralized infrastructure. This makes them indispensable for edge-based applications like\\ndynamic resource allocation, real-time monitoring, and edge device coordination, ensuring that next-generation\\nnetworks achieve the agility and efficiency required to meet evolving demands.\\n200\\nReferences\\n[1] M. E. Morocho Cayamcela and W. Lim, “Artificial intelligence in 5g technology: A survey,” in 2018', 'NR/5GC \\nNR connected to 5GC \\nPCell \\nPrimary Cell \\nPDCP \\nPacket Data Convergence Protocol \\nPDU \\nProtocol Data Unit \\nPLMN \\nPublic Land Mobile Network \\nPSCell \\nPrimary SCG Cell \\nPWS \\nPublic Warning System \\nQoS \\nQuality of Service \\nRAN \\nRadio Access Network \\nRAT \\nRadio Access Technology \\nRLC \\nRadio Link Control \\nRNA \\nRAN-based Notification Area \\nRNTI \\nRadio Network Temporary Identifier \\nROHC \\nRobust Header Compression \\nRRC \\nRadio Resource Control \\nRS \\nReference Signal', 'and Networking, vol. 16, no. 6, pp. 681–694, 2024.\\n[393] K. Qiu, S. Bakirtzis, I. Wassell, H. Song, J. Zhang, and K. Wang, “Large language model-based wireless\\nnetwork design,” IEEE Wireless Communications Letters, vol. 13, no. 12, pp. 3340–3344, 2024.\\n[394] M. Agiwal, A. Roy, and N. Saxena, “Next generation 5G wireless networks: A comprehensive survey,”\\nIEEE Commun. Surveys Tuts., vol. 18, no. 3, pp. 1617–1655, 3rd Quart., 2016.\\n[395] D. Soldani and A. Manzalini, “Horizon 2020 and beyond: On the 5g operating system for a true digital\\nsociety,” IEEE Vehicular Technology Magazine, vol. 10, no. 1, pp. 32–42, 2015.\\n[396] Y. Singh, “Comparison of okumura, hata and cost-231 models on the basis of path loss and signal strength,”\\nInternational journal of computer applications, vol. 59, no. 11, 2012.\\n[397] Z. Yun and M. F. Iskander, “Ray tracing for radio propagation modeling: Principles and applications,” IEEE\\naccess, vol. 3, pp. 1089–1100, 2015.', 'niques such as parameter-efficient fine-tuning and split edge learning have been proposed to mitigate these\\nchallenges by reducing the computational load required for training LLMs at the edge of the network [333].\\n• Future Directions and Challenges\\nAs telecommunication networks continue to evolve, the role of distributed architectures, big data analytics,\\nand AI models like LLMs will only grow. However, there are several challenges that need to be addressed\\nto fully realize the potential of these technologies.\\nOne major challenge is the scalability of distributed architectures. Telecommunications networks are con-\\n120\\nstantly expanding in size and complexity, with the introduction of new technologies like 5G and edge com-\\nputing. Managing these networks requires scalable solutions that can handle vast amounts of data and\\nsupport real-time decision-making across multiple nodes [332].', 'PMLR, 2017, pp.\\n1126–1135. [Online]. Available: https://arxiv.org/abs/1703.03400\\n[505] R. Li, Z. Zhao, Q. Sun, C.-L. I, C. Yang, X. Chen, M. Zhao, and H. Zhang, “Deep reinforcement learning\\nfor resource management in network slicing,” IEEE Access, vol. 6, pp. 74 429–74 441, 2018.\\n[506] Y. Kim and H. Lim, “Multi-agent reinforcement learning-based resource management for end-to-end net-\\nwork slicing,” IEEE Access, vol. 9, pp. 56 178–56 190, 2021.\\n[507] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Which ml model to choose? experimental evalua-\\ntion for a beyond-5g traffic steering case,” in ICC 2023-IEEE International Conference on Communications.\\nIEEE, 2023, pp. 5185–5190.\\n[508] H.\\nD.\\nXατζηστεϕαν´ιδης,\\n“σχεδιασµ ´oς\\nκαι\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.', 'as the native part of the O-RAN architecture will give new opportunities for xApp and rApp designers to better\\ncreate their own optimization functions.\\n13.2.3\\nLTM on 6G Digital Twin\\nDigital Twin (DT) technology, augmented with LTMs, represents a transformative opportunity for advancing the\\n5G industry into the 6G era. By creating real-time virtual replicas of network infrastructures—including devices,\\nbase stations, and communication links—DTs enable precise simulation and optimization of network performance.\\nThe integration of LTMs within DTs enhances their intelligence and adaptability, empowering networks to not\\nonly mirror physical systems but also analyze, predict, and proactively resolve challenges with unprecedented\\nefficiency. This synergy is essential for managing the complexity of 6G networks, characterized by ultra-dense\\nconnectivity, intelligent automation, and dynamic resource allocation.']}}\n",
      "{'generate_response': {'response': '5G Network Slicing refers to a technology that allows multiple independent networks (slices) to be created on top of a single 5G network infrastructure. Each slice represents a virtualized network, with its own set of resources, such as radio access, core network, and transport network, which are tailored for specific use cases or applications.\\n\\nNetwork slicing enables different types of services or applications to share the same physical infrastructure, but with their own dedicated resources and management policies. This allows for greater flexibility, scalability, and efficiency in managing the 5G network.\\n\\nFor example, a slice might be dedicated to a specific application, such as virtual reality (VR) or augmented reality (AR), which requires a high-bandwidth and low-latency connection. Another slice might be dedicated to a different application, such as IoT (Internet of Things) devices, which require a lower-bandwidth and more reliable connection.\\n\\nNetwork slicing is an important feature of 5G networks, as it enables the creation of customized networks for specific use cases, and allows for greater flexibility and scalability in managing the network.'}}\n",
      "{'update_memory': {'chat_history': \"Human: What is 5G Network slicing?\\nAI: 5G Network Slicing refers to a technology used in 5G wireless networks that allows multiple independent networks (slices) to be created on top of the same physical infrastructure. Each slice provides a unique set of characteristics, such as different bandwidth, latency, and quality of service (QoS), tailored to specific use cases or applications.\\n\\nNetwork slicing enables various scenarios, including:\\n\\n1. **Multiple independent services**: Different service providers can operate on the same network without interfering with each other.\\n2. **Customizable QoS**: Each slice can provide a unique level of service, such as high-speed data for online gaming or low-latency for real-time video streaming.\\n3. **Improved resource utilization**: Slices can be optimized to use specific resources, like bandwidth or computational power, more efficiently.\\n4. **Enhanced security and isolation**: Slices are isolated from each other, providing a secure environment for sensitive applications.\\n\\nThe concept of network slicing is essential for 5G's success, as it allows for the creation of multiple virtual networks that can be tailored to specific use cases, making it easier to manage and optimize the performance of the overall network.\\nHuman: What is 5G Network slicing?\\nAI: 5G Network Slicing refers to a technology that allows multiple independent networks (slices) to be created on top of a single 5G network infrastructure. Each slice represents a virtualized network, with its own set of resources, such as radio access, core network, and transport network, which are tailored for specific use cases or applications.\\n\\nNetwork slicing enables different types of services or applications to share the same physical infrastructure, but with their own dedicated resources and management policies. This allows for greater flexibility, scalability, and efficiency in managing the 5G network.\\n\\nFor example, a slice might be dedicated to a specific application, such as virtual reality (VR) or augmented reality (AR), which requires a high-bandwidth and low-latency connection. Another slice might be dedicated to a different application, such as IoT (Internet of Things) devices, which require a lower-bandwidth and more reliable connection.\\n\\nNetwork slicing is an important feature of 5G networks, as it enables the creation of customized networks for specific use cases, and allows for greater flexibility and scalability in managing the network.\"}}\n"
     ]
    }
   ],
   "source": [
    "# Örnek ççalıştırmak için\n",
    "test_input = {\"user_query\": \"What is 5G Network slicing?\"}\n",
    "for s in rag_graph.stream(test_input):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aaa51-ecaa-465e-94b2-6697a0135ef1",
   "metadata": {},
   "source": [
    "## stategraph with grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d824eadf-6dc4-468b-92a9-a7dc023f3fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': {'user_query': 'What is 5G Network slicing?'}}\n",
      "{'retrieve': {'retrieved_docs': ['slicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling\\nremote hands-on computer networking education: the nitos testbed approach,” in 2023 IEEE Integrated\\nSTEM Education Conference (ISEC).\\nIEEE, 2023, pp. 132–138.\\n[513] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ue statistics time-series (cqi) in lte networks,”\\n2022.\\n[514] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ue network traffic time-series (applications,\\nthroughput, latency, cqi) in lte/5g networks,” IEEE Dataport, 2022.\\n[515] D. Spathis and F. Kawsar, “The first step is the hardest: Pitfalls of representing and tokenizing temporal\\ndata for large language models,” Journal of the American Medical Informatics Association, vol. 31, no. 9,\\npp. 2151–2158, 2024.', 'actions involved in modifying network configuration files and deployments. This study presents experiments and\\nresearch on the automated generation of commit messages in the context of 5G network deployment.\\n135\\nIn contemporary network management, the concept of Network as Code (NAC) is gaining traction. NAC funda-\\nmentally applies software development methodologies to manage and configure network devices and services. It\\nenables the management of network configurations through version control systems and the implementation of\\nautomated tools and processes, similar to software development operations. This methodology not only boosts\\noperational efficiency and minimizes human errors but also speeds up the deployment of network configurations.\\nThe adoption of NAC greatly enhances the consistency, traceability, and replicability of network configurations,\\nproviding exceptional flexibility and control in network operations.', 'και\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.\\n[509] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ml-based traffic steering for heterogeneous ultra-\\ndense beyond-5g networks,” in 2023 IEEE Wireless Communications and Networking Conference (WCNC).\\nIEEE, 2023, pp. 1–6.\\n[510] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ai-driven service-aware real-time slicing for\\nbeyond 5g networks,” in IEEE INFOCOM 2022-IEEE Conference on Computer Communications Work-\\n234\\nshops (INFOCOM WKSHPS).\\nIEEE, 2022, pp. 1–6.\\n[511] T. Tsourdinis, I. Chatzistefanidis, N. Makris, T. Korakis, N. Nikaein, and S. Fdida, “Service-aware real-time\\nslicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling', 'Additionally, it covers 3GPP standards for mobile networks such as 4G LTE and 5G, which are critical for modern\\nmobile communication. ETSI standards from the European Telecommunications Standards Institute for mobile\\nand broadband networks, and IETF standards for internet protocols like TCP/IP, are also within its scope. Further-\\nmore, StandardGPT can address 5G and next-generation network standards, along with network security standards\\n(e.g., ISO/IEC 27033) and compliance with national regulatory frameworks like those from the FCC in the U.S.\\nThis broad coverage makes StandardGPT a useful tool for navigating both technical and regulatory aspects of the\\ntelecommunications industry.\\nOpenAI also proposed a more specific model for telecommunication: TelecomGPT [491]. TelecomGPT inludes\\ntelecommunication knowledge but it is also capable of doing tasks such as VoIP and analyzing SIP logs, help', 'into the building-block approach for AI factories.\\n7.5\\nConvergence of AI and RAN\\nThe advent and adoption of 5G technology has introduced a new level of speed and capability in networks, en-\\nabling them to deliver not only voice, but also Internet-based services, data, and streaming. This generalization of\\ntelecom networks into a data service network allows the introduction of LLM-driven services. A further benefit\\ncomes from the flexibility built into 5G network architecture, allowing different components to be strategically\\ndistributed in different locations, from the radio unit (RU) site at the very edge, to DU sites (such as MSOs), to\\nthe CU which has more space and power available. This flexibility also means that components can be decoupled,\\nsuch that following Open RAN (O-RAN) specifications enables a merger of the best solutions for both RAN and\\nAI processing while meeting latency demands [247]. This flexibility allows smaller footprints for AI inference to', 'policy generation, making them a practical solution for real-world use cases.\\nSLMs are particularly well-suited for low-latency and resource-constrained environments, such as those found at\\nthe network edge. As edge computing becomes a cornerstone of 6G networks, the ability to deploy intelligent\\nmodels close to the end-user is essential for minimizing latency and improving service responsiveness. SLMs can\\nefficiently operate in these decentralized settings, enabling localized decision-making and real-time adjustments\\nwithout relying on centralized infrastructure. This makes them indispensable for edge-based applications like\\ndynamic resource allocation, real-time monitoring, and edge device coordination, ensuring that next-generation\\nnetworks achieve the agility and efficiency required to meet evolving demands.\\n200\\nReferences\\n[1] M. E. Morocho Cayamcela and W. Lim, “Artificial intelligence in 5g technology: A survey,” in 2018', 'NR/5GC \\nNR connected to 5GC \\nPCell \\nPrimary Cell \\nPDCP \\nPacket Data Convergence Protocol \\nPDU \\nProtocol Data Unit \\nPLMN \\nPublic Land Mobile Network \\nPSCell \\nPrimary SCG Cell \\nPWS \\nPublic Warning System \\nQoS \\nQuality of Service \\nRAN \\nRadio Access Network \\nRAT \\nRadio Access Technology \\nRLC \\nRadio Link Control \\nRNA \\nRAN-based Notification Area \\nRNTI \\nRadio Network Temporary Identifier \\nROHC \\nRobust Header Compression \\nRRC \\nRadio Resource Control \\nRS \\nReference Signal', 'and Networking, vol. 16, no. 6, pp. 681–694, 2024.\\n[393] K. Qiu, S. Bakirtzis, I. Wassell, H. Song, J. Zhang, and K. Wang, “Large language model-based wireless\\nnetwork design,” IEEE Wireless Communications Letters, vol. 13, no. 12, pp. 3340–3344, 2024.\\n[394] M. Agiwal, A. Roy, and N. Saxena, “Next generation 5G wireless networks: A comprehensive survey,”\\nIEEE Commun. Surveys Tuts., vol. 18, no. 3, pp. 1617–1655, 3rd Quart., 2016.\\n[395] D. Soldani and A. Manzalini, “Horizon 2020 and beyond: On the 5g operating system for a true digital\\nsociety,” IEEE Vehicular Technology Magazine, vol. 10, no. 1, pp. 32–42, 2015.\\n[396] Y. Singh, “Comparison of okumura, hata and cost-231 models on the basis of path loss and signal strength,”\\nInternational journal of computer applications, vol. 59, no. 11, 2012.\\n[397] Z. Yun and M. F. Iskander, “Ray tracing for radio propagation modeling: Principles and applications,” IEEE\\naccess, vol. 3, pp. 1089–1100, 2015.', 'niques such as parameter-efficient fine-tuning and split edge learning have been proposed to mitigate these\\nchallenges by reducing the computational load required for training LLMs at the edge of the network [333].\\n• Future Directions and Challenges\\nAs telecommunication networks continue to evolve, the role of distributed architectures, big data analytics,\\nand AI models like LLMs will only grow. However, there are several challenges that need to be addressed\\nto fully realize the potential of these technologies.\\nOne major challenge is the scalability of distributed architectures. Telecommunications networks are con-\\n120\\nstantly expanding in size and complexity, with the introduction of new technologies like 5G and edge com-\\nputing. Managing these networks requires scalable solutions that can handle vast amounts of data and\\nsupport real-time decision-making across multiple nodes [332].', 'PMLR, 2017, pp.\\n1126–1135. [Online]. Available: https://arxiv.org/abs/1703.03400\\n[505] R. Li, Z. Zhao, Q. Sun, C.-L. I, C. Yang, X. Chen, M. Zhao, and H. Zhang, “Deep reinforcement learning\\nfor resource management in network slicing,” IEEE Access, vol. 6, pp. 74 429–74 441, 2018.\\n[506] Y. Kim and H. Lim, “Multi-agent reinforcement learning-based resource management for end-to-end net-\\nwork slicing,” IEEE Access, vol. 9, pp. 56 178–56 190, 2021.\\n[507] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Which ml model to choose? experimental evalua-\\ntion for a beyond-5g traffic steering case,” in ICC 2023-IEEE International Conference on Communications.\\nIEEE, 2023, pp. 5185–5190.\\n[508] H.\\nD.\\nXατζηστεϕαν´ιδης,\\n“σχεδιασµ ´oς\\nκαι\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.']}}\n",
      "{'generate_response': {'response': '5G Network Slicing refers to the concept of creating virtual networks within a physical network infrastructure, allowing multiple independent networks (slices) to coexist on the same underlying physical resources. Each slice represents a unique network configuration with its own set of parameters, such as radio access technology, transmission power, and bearer capacity.\\n\\nNetwork slicing is an essential feature of 5G networks, enabling a wide range of use cases, including:\\n\\n1. **Multi-tenancy**: Multiple independent networks can share the same physical infrastructure, reducing costs and increasing efficiency.\\n2. **Customization**: Each slice can be tailored to meet specific requirements, such as different quality of service (QoS) levels or security standards.\\n3. **Scalability**: Slices can be scaled up or down independently, allowing for more efficient use of resources.\\n4. **Interoperability**: Different slices can communicate with each other seamlessly, facilitating the creation of hybrid networks.\\n\\nNetwork slicing is achieved through the use of advanced network management techniques, such as software-defined networking (SDN) and network function virtualization (NFV). These technologies enable the creation of virtualized network functions, which can be easily replicated and managed across multiple slices.\\n\\nThe benefits of 5G Network Slicing include:\\n\\n1. **Increased flexibility**: Multiple independent networks can coexist on the same physical infrastructure.\\n2. **Improved efficiency**: Resources are utilized more efficiently, reducing costs and increasing scalability.\\n3. **Enhanced security**: Each slice can be configured with specific security policies, ensuring a higher level of security for each network.\\n\\nHowever, implementing 5G Network Slicing also presents several challenges, including:\\n\\n1. **Complexity**: Managing multiple independent networks requires advanced network management skills and tools.\\n2. **Scalability**: Scaling individual slices while maintaining the overall performance of the network can be challenging.\\n3. **Interoperability**: Ensuring seamless communication between different slices is essential but can be a complex task.\\n\\nOverall, 5G Network Slicing has the potential to revolutionize the way networks are designed, managed, and operated, enabling a wide range of innovative use cases and applications that were previously not possible with traditional network architectures.'}}\n",
      "Grader Output: Here are my evaluations of the benefits and challenges of 5G Network Slicing:\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "1. **Increased flexibility**: 8/10\n",
      "The ability to create multiple independent networks on the same physical infrastructure is a significant advantage, allowing for greater flexibility in network design and operation.\n",
      "\n",
      "2. **Improved efficiency**: 9/10\n",
      "By utilizing resources more efficiently, 5G Network Slicing can reduce costs and increase scalability, making it an attractive solution for organizations with diverse networking needs.\n",
      "\n",
      "3. **Enhanced security**: 7/10\n",
      "While each slice can be configured with specific security policies, ensuring seamless communication between different slices is a complex task that may require additional effort to implement.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "1. **Complexity**: 6/10\n",
      "Managing multiple independent networks requires advanced network management skills and tools, but the complexity of this challenge can be mitigated through proper planning and resource allocation.\n",
      "\n",
      "2. **Scalability**: 8/10\n",
      "Scaling individual slices while maintaining overall network performance is a significant challenge that requires careful consideration of traffic distribution and QoS policies.\n",
      "\n",
      "3. **Interoperability**: 5/10\n",
      "Ensuring seamless communication between different slices is essential, but this challenge may require additional effort to implement, such as developing custom protocols or integrating with existing systems.\n",
      "\n",
      "Overall, the benefits of 5G Network Slicing outweigh the challenges, making it a promising technology for creating flexible and efficient networks. However, careful planning and resource allocation are necessary to overcome the complexities associated with managing multiple independent networks.\n",
      "{'evaluate_response': {'evaluation_score': 0.0}}\n",
      "{'revise_response': {'response': 'Unfortunately, the provided text does not explicitly define 5G Network Slicing. However, based on the context and related information, I can provide a general explanation of what 5G Network Slicing is.\\n\\n5G Network Slicing refers to a technology used in 5G networks that enables multiple independent networks (slices) to be created within a single physical network. Each slice represents a specific use case or application, such as a dedicated IoT network, a high-speed data network, or a low-latency real-time application.\\n\\nThink of it like a virtual partitioning of the physical network into separate, self-contained environments, each with its own set of rules, policies, and performance characteristics. This allows multiple independent services to coexist on the same network infrastructure, improving efficiency, scalability, and resource utilization.\\n\\nNetwork Slicing is an important concept in 5G networks, as it enables service providers to offer a wider range of services and applications to their customers while also reducing costs and improving overall network efficiency.\\n\\nIn summary, 5G Network Slicing is a technology that allows multiple independent networks to be created within a single physical network, each with its own set of rules and performance characteristics.'}}\n",
      "{'generate_response': {'response': 'Unfortunately, the provided text does not explicitly define what 5G network slicing is. However, based on general knowledge and context, I can provide an answer:\\n\\n**5G Network Slicing**: In the context of 5G networks, slicing refers to a concept where multiple independent, virtualized networks (or \"slices\") are created on top of a shared physical infrastructure. Each slice represents a customized network with its own set of resources, policies, and services.\\n\\nThink of it like a multi-tenant building, where each tenant has their own separate apartment with unique amenities and rules. In the same way, 5G network slicing allows multiple independent networks to coexist on the same physical infrastructure, each with its own characteristics, such as different bandwidth allocation, latency, and security settings.\\n\\nNetwork slicing enables various use cases, including:\\n\\n* Differentiating between enterprise and consumer networks\\n* Providing customized services for specific industries (e.g., healthcare, finance)\\n* Supporting multiple applications with varying performance requirements\\n* Enhancing network agility and flexibility\\n\\nIn summary, 5G network slicing is a technique that allows multiple virtualized networks to run on top of a shared physical infrastructure, enabling greater flexibility, customization, and efficiency in managing different types of traffic and services.'}}\n",
      "Grader Output: I'll do my best to provide scores based on general knowledge about 5G network slicing. Keep in mind that these scores are subjective and may not reflect the authors' intentions or expertise.\n",
      "\n",
      "**Criteria:** Definition, Purpose, Benefits, Challenges, Future Directions\n",
      "\n",
      "1. **Definition**: 0.6\n",
      "The text doesn't explicitly define what 5G network slicing is, but I'll provide a score based on general knowledge. A score of 0.6 indicates that the definition is partially clear, but it's not thoroughly explained.\n",
      "\n",
      "2. **Purpose**: 0.8\n",
      "Based on general knowledge, I believe that 5G network slicing enables multiple independent networks to coexist on the same physical infrastructure. This allows for greater flexibility, customization, and efficiency in managing different types of traffic and services. A score of 0.8 indicates that the purpose is generally well-understood.\n",
      "\n",
      "3. **Benefits**: 0.9\n",
      "The benefits of 5G network slicing are widely recognized, including improved network agility, enhanced service differentiation, and increased operational efficiency. A score of 0.9 indicates that these benefits are well-established and widely accepted.\n",
      "\n",
      "4. **Challenges**: 0.7\n",
      "While challenges exist in implementing and managing 5G network slicing (e.g., scalability, complexity, and security concerns), they are not as well-defined or universally acknowledged as the benefits. A score of 0.7 indicates that these challenges are acknowledged but may require further clarification.\n",
      "\n",
      "5. **Future Directions**: 0.8\n",
      "Research on future directions for 5G network slicing is ongoing, exploring topics such as AI-powered management, edge computing, and hybrid architectures. A score of 0.8 indicates that the field is actively developing new ideas and technologies to address emerging challenges.\n",
      "\n",
      "Please note that these scores are subjective and may not reflect the authors' intentions or expertise.\n",
      "{'evaluate_response': {'evaluation_score': 0.0}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Örnek çalıştırma\u001b[39;00m\n\u001b[1;32m    138\u001b[0m test_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 5G Network slicing?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m rag_graph\u001b[38;5;241m.\u001b[39mstream(test_input):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1724\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1724\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1725\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1726\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1727\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1728\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1729\u001b[0m         ):\n\u001b[1;32m   1730\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     run_with_retry(\n\u001b[1;32m    231\u001b[0m         t,\n\u001b[1;32m    232\u001b[0m         retry_policy,\n\u001b[1;32m    233\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    234\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    235\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    236\u001b[0m         },\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:495\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    492\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:259\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 259\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[11], line 105\u001b[0m, in \u001b[0;36mrevise_response_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrevise_response_node\u001b[39m(state: AgentState):\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: generate_response_node(state)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "Cell \u001b[0;32mIn[11], line 77\u001b[0m, in \u001b[0;36mgenerate_response_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     67\u001b[0m combined_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     68\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124mYou are a telecom assistant. Your answers should be based on the retrieved context below.\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124mAnswer:\u001b[39m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 77\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(prompt)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: response}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[1;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[1;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    396\u001b[0m         )\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    399\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:760\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    754\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    758\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    759\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:963\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    950\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    951\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    961\u001b[0m         )\n\u001b[1;32m    962\u001b[0m     ]\n\u001b[0;32m--> 963\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[1;32m    964\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    965\u001b[0m     )\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_core/language_models/llms.py:784\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    776\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    781\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 784\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    785\u001b[0m                 prompts,\n\u001b[1;32m    786\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    787\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[1;32m    788\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    790\u001b[0m             )\n\u001b[1;32m    791\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    793\u001b[0m         )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_community/llms/ollama.py:437\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 437\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[1;32m    438\u001b[0m         prompt,\n\u001b[1;32m    439\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    440\u001b[0m         images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[1;32m    441\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m    442\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_community/llms/ollama.py:349\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    342\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    348\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    351\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langchain_community/llms/ollama.py:194\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    192\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    193\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    195\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    196\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    197\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    199\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    871\u001b[0m ):\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/utils.py:572\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    571\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    573\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_chunk_length()\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline()  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/socket.py:708\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Grader için Prompt\n",
    "GRADER_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expert evaluator. Grade the response based on the following criteria:\n",
    "1. Groundedness: Does the response fully address the question using the provided context?\n",
    "2. Answer Relevance: Is the response relevant and directly answering the question?\n",
    "3. Context Relevance: Is the response actually correct based on the context?\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Response:\n",
    "{response}\n",
    "\n",
    "---\n",
    "\n",
    "Provide a float score from 0 to 1 for each criteria and a short explanation for the score.\n",
    "\"\"\"\n",
    "\n",
    "class Grader:\n",
    "    def __init__(self):\n",
    "        self.model = Ollama(model=\"llama3.2\")  # Grading LLM\n",
    "\n",
    "    def grade(self, context: str, question: str, response: str) -> str:\n",
    "        prompt_template = ChatPromptTemplate.from_template(GRADER_PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(\n",
    "            context=context, question=question, response=response\n",
    "        )\n",
    "        return self.model.invoke(prompt)\n",
    "\n",
    "# AgentState yapısı\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    retrieved_docs: List[str]\n",
    "    response: str\n",
    "    chat_history: str\n",
    "    evaluation_score: float\n",
    "\n",
    "# Hafıza ve vektör veritabanı yükleme\n",
    "DB_NAME = \"vector-db\"\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "memory = ConversationBufferMemory()\n",
    "grader = Grader()\n",
    "\n",
    "# Düğümler (Nodes)\n",
    "def user_input_node(state: AgentState):\n",
    "    return {\"user_query\": state[\"user_query\"]}\n",
    "\n",
    "def retrieve_node(state: AgentState):\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"user_query\"], k=10)\n",
    "    return {\"retrieved_docs\": [doc.page_content for doc in retrieved_docs]}\n",
    "\n",
    "def generate_response_node(state: AgentState):\n",
    "    combined_context = \"\\n\".join(state[\"retrieved_docs\"])\n",
    "    prompt = f\"\"\"\n",
    "    You are a telecom assistant. Your answers should be based on the retrieved context below.\n",
    "    \n",
    "    Context: {combined_context}\n",
    "    \n",
    "    Question: {state['user_query']}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"response\": response}\n",
    "\n",
    "def evaluate_response_node(state: AgentState):\n",
    "    evaluation_result = grader.grade(\n",
    "        context=\"\\n\".join(state[\"retrieved_docs\"]),\n",
    "        question=state[\"user_query\"],\n",
    "        response=state[\"response\"]\n",
    "    )\n",
    "    \n",
    "    print(\"Grader Output:\", evaluation_result)  # Debug için çıktıyı yazdır\n",
    "    \n",
    "    score_values = []\n",
    "    score_lines = evaluation_result.split(\"\\n\")\n",
    "    \n",
    "    for line in score_lines:\n",
    "        if any(keyword in line for keyword in [\"Overall Score\", \"Final Score\", \"Total Score\"]):\n",
    "            try:\n",
    "                score = float(line.split(\":\")[1].strip().replace('*', ''))\n",
    "                score_values.append(score)\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "    \n",
    "    final_score = max(score_values, default=0.0)  # Eğer hiç uygun skor bulunmazsa 0.0 ata\n",
    "    return {\"evaluation_score\": final_score}\n",
    "\n",
    "def revise_response_node(state: AgentState):\n",
    "    if state[\"evaluation_score\"] < 0.5:\n",
    "        return {\"response\": generate_response_node(state)[\"response\"]}\n",
    "    return {}\n",
    "\n",
    "def update_memory_node(state: AgentState):\n",
    "    memory.save_context({\"input\": state[\"user_query\"]}, {\"output\": state[\"response\"]})\n",
    "    return {\"chat_history\": memory.load_memory_variables({}).get(\"history\", \"\")}\n",
    "\n",
    "# StateGraph oluşturma\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"user_input\", user_input_node)\n",
    "builder.add_node(\"retrieve\", retrieve_node)\n",
    "builder.add_node(\"generate_response\", generate_response_node)\n",
    "builder.add_node(\"evaluate_response\", evaluate_response_node)\n",
    "builder.add_node(\"revise_response\", revise_response_node)\n",
    "builder.add_node(\"update_memory\", update_memory_node)\n",
    "\n",
    "# İş akışını belirleme\n",
    "builder.set_entry_point(\"user_input\")\n",
    "builder.add_edge(\"user_input\", \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"generate_response\")\n",
    "builder.add_edge(\"generate_response\", \"evaluate_response\")\n",
    "builder.add_conditional_edges(\n",
    "    \"evaluate_response\", \n",
    "    lambda state: \"revise_response\" if state[\"evaluation_score\"] < 0.5 else \"update_memory\",\n",
    "    {\"revise_response\": \"revise_response\", \"update_memory\": \"update_memory\"}\n",
    ")\n",
    "builder.add_edge(\"revise_response\", \"generate_response\")\n",
    "builder.add_edge(\"update_memory\", END)\n",
    "\n",
    "# Graph oluştur\n",
    "rag_graph = builder.compile()\n",
    "\n",
    "# Örnek çalıştırma\n",
    "test_input = {\"user_query\": \"What is 5G Network slicing?\"}\n",
    "for s in rag_graph.stream(test_input):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25befafe-3cbf-4726-8d09-ccd53338c055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_input': {'user_query': 'What is 5G Network slicing?'}}\n",
      "{'retrieve': {'retrieved_docs': ['slicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling\\nremote hands-on computer networking education: the nitos testbed approach,” in 2023 IEEE Integrated\\nSTEM Education Conference (ISEC).\\nIEEE, 2023, pp. 132–138.\\n[513] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ue statistics time-series (cqi) in lte networks,”\\n2022.\\n[514] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ue network traffic time-series (applications,\\nthroughput, latency, cqi) in lte/5g networks,” IEEE Dataport, 2022.\\n[515] D. Spathis and F. Kawsar, “The first step is the hardest: Pitfalls of representing and tokenizing temporal\\ndata for large language models,” Journal of the American Medical Informatics Association, vol. 31, no. 9,\\npp. 2151–2158, 2024.', 'actions involved in modifying network configuration files and deployments. This study presents experiments and\\nresearch on the automated generation of commit messages in the context of 5G network deployment.\\n135\\nIn contemporary network management, the concept of Network as Code (NAC) is gaining traction. NAC funda-\\nmentally applies software development methodologies to manage and configure network devices and services. It\\nenables the management of network configurations through version control systems and the implementation of\\nautomated tools and processes, similar to software development operations. This methodology not only boosts\\noperational efficiency and minimizes human errors but also speeds up the deployment of network configurations.\\nThe adoption of NAC greatly enhances the consistency, traceability, and replicability of network configurations,\\nproviding exceptional flexibility and control in network operations.', 'και\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.\\n[509] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Ml-based traffic steering for heterogeneous ultra-\\ndense beyond-5g networks,” in 2023 IEEE Wireless Communications and Networking Conference (WCNC).\\nIEEE, 2023, pp. 1–6.\\n[510] T. Tsourdinis, I. Chatzistefanidis, N. Makris, and T. Korakis, “Ai-driven service-aware real-time slicing for\\nbeyond 5g networks,” in IEEE INFOCOM 2022-IEEE Conference on Computer Communications Work-\\n234\\nshops (INFOCOM WKSHPS).\\nIEEE, 2022, pp. 1–6.\\n[511] T. Tsourdinis, I. Chatzistefanidis, N. Makris, T. Korakis, N. Nikaein, and S. Fdida, “Service-aware real-time\\nslicing for virtualized beyond 5g networks,” Computer Networks, vol. 247, p. 110445, 2024.\\n[512] N. Makris, V. Passas, A. Apostolaras, T. Tsourdinis, I. Chatzistefanidis, and T. Korakis, “On enabling', 'Additionally, it covers 3GPP standards for mobile networks such as 4G LTE and 5G, which are critical for modern\\nmobile communication. ETSI standards from the European Telecommunications Standards Institute for mobile\\nand broadband networks, and IETF standards for internet protocols like TCP/IP, are also within its scope. Further-\\nmore, StandardGPT can address 5G and next-generation network standards, along with network security standards\\n(e.g., ISO/IEC 27033) and compliance with national regulatory frameworks like those from the FCC in the U.S.\\nThis broad coverage makes StandardGPT a useful tool for navigating both technical and regulatory aspects of the\\ntelecommunications industry.\\nOpenAI also proposed a more specific model for telecommunication: TelecomGPT [491]. TelecomGPT inludes\\ntelecommunication knowledge but it is also capable of doing tasks such as VoIP and analyzing SIP logs, help', 'into the building-block approach for AI factories.\\n7.5\\nConvergence of AI and RAN\\nThe advent and adoption of 5G technology has introduced a new level of speed and capability in networks, en-\\nabling them to deliver not only voice, but also Internet-based services, data, and streaming. This generalization of\\ntelecom networks into a data service network allows the introduction of LLM-driven services. A further benefit\\ncomes from the flexibility built into 5G network architecture, allowing different components to be strategically\\ndistributed in different locations, from the radio unit (RU) site at the very edge, to DU sites (such as MSOs), to\\nthe CU which has more space and power available. This flexibility also means that components can be decoupled,\\nsuch that following Open RAN (O-RAN) specifications enables a merger of the best solutions for both RAN and\\nAI processing while meeting latency demands [247]. This flexibility allows smaller footprints for AI inference to', 'policy generation, making them a practical solution for real-world use cases.\\nSLMs are particularly well-suited for low-latency and resource-constrained environments, such as those found at\\nthe network edge. As edge computing becomes a cornerstone of 6G networks, the ability to deploy intelligent\\nmodels close to the end-user is essential for minimizing latency and improving service responsiveness. SLMs can\\nefficiently operate in these decentralized settings, enabling localized decision-making and real-time adjustments\\nwithout relying on centralized infrastructure. This makes them indispensable for edge-based applications like\\ndynamic resource allocation, real-time monitoring, and edge device coordination, ensuring that next-generation\\nnetworks achieve the agility and efficiency required to meet evolving demands.\\n200\\nReferences\\n[1] M. E. Morocho Cayamcela and W. Lim, “Artificial intelligence in 5g technology: A survey,” in 2018', 'NR/5GC \\nNR connected to 5GC \\nPCell \\nPrimary Cell \\nPDCP \\nPacket Data Convergence Protocol \\nPDU \\nProtocol Data Unit \\nPLMN \\nPublic Land Mobile Network \\nPSCell \\nPrimary SCG Cell \\nPWS \\nPublic Warning System \\nQoS \\nQuality of Service \\nRAN \\nRadio Access Network \\nRAT \\nRadio Access Technology \\nRLC \\nRadio Link Control \\nRNA \\nRAN-based Notification Area \\nRNTI \\nRadio Network Temporary Identifier \\nROHC \\nRobust Header Compression \\nRRC \\nRadio Resource Control \\nRS \\nReference Signal', 'and Networking, vol. 16, no. 6, pp. 681–694, 2024.\\n[393] K. Qiu, S. Bakirtzis, I. Wassell, H. Song, J. Zhang, and K. Wang, “Large language model-based wireless\\nnetwork design,” IEEE Wireless Communications Letters, vol. 13, no. 12, pp. 3340–3344, 2024.\\n[394] M. Agiwal, A. Roy, and N. Saxena, “Next generation 5G wireless networks: A comprehensive survey,”\\nIEEE Commun. Surveys Tuts., vol. 18, no. 3, pp. 1617–1655, 3rd Quart., 2016.\\n[395] D. Soldani and A. Manzalini, “Horizon 2020 and beyond: On the 5g operating system for a true digital\\nsociety,” IEEE Vehicular Technology Magazine, vol. 10, no. 1, pp. 32–42, 2015.\\n[396] Y. Singh, “Comparison of okumura, hata and cost-231 models on the basis of path loss and signal strength,”\\nInternational journal of computer applications, vol. 59, no. 11, 2012.\\n[397] Z. Yun and M. F. Iskander, “Ray tracing for radio propagation modeling: Principles and applications,” IEEE\\naccess, vol. 3, pp. 1089–1100, 2015.', 'niques such as parameter-efficient fine-tuning and split edge learning have been proposed to mitigate these\\nchallenges by reducing the computational load required for training LLMs at the edge of the network [333].\\n• Future Directions and Challenges\\nAs telecommunication networks continue to evolve, the role of distributed architectures, big data analytics,\\nand AI models like LLMs will only grow. However, there are several challenges that need to be addressed\\nto fully realize the potential of these technologies.\\nOne major challenge is the scalability of distributed architectures. Telecommunications networks are con-\\n120\\nstantly expanding in size and complexity, with the introduction of new technologies like 5G and edge com-\\nputing. Managing these networks requires scalable solutions that can handle vast amounts of data and\\nsupport real-time decision-making across multiple nodes [332].', 'PMLR, 2017, pp.\\n1126–1135. [Online]. Available: https://arxiv.org/abs/1703.03400\\n[505] R. Li, Z. Zhao, Q. Sun, C.-L. I, C. Yang, X. Chen, M. Zhao, and H. Zhang, “Deep reinforcement learning\\nfor resource management in network slicing,” IEEE Access, vol. 6, pp. 74 429–74 441, 2018.\\n[506] Y. Kim and H. Lim, “Multi-agent reinforcement learning-based resource management for end-to-end net-\\nwork slicing,” IEEE Access, vol. 9, pp. 56 178–56 190, 2021.\\n[507] I. Chatzistefanidis, N. Makris, V. Passas, and T. Korakis, “Which ml model to choose? experimental evalua-\\ntion for a beyond-5g traffic steering case,” in ICC 2023-IEEE International Conference on Communications.\\nIEEE, 2023, pp. 5185–5190.\\n[508] H.\\nD.\\nXατζηστεϕαν´ιδης,\\n“σχεδιασµ ´oς\\nκαι\\nυλoπo´ιηση\\nαλγoρ´ιθµων\\nτεχνητ ´ης\\nνoηµoσ ´ννης για την χρoνoδρoµoλ ´oγηση σε πραγµατικ ´o χρ ´oνo ασ ´νρµατων πελατ ´ων\\nδικτ ´νων 5ης και π ´εραν της 5ης γενι ´ας,” B.S. thesis, 2020.']}}\n",
      "{'generate_response': {'response': '5G Network Slicing refers to the process of creating multiple virtual networks (slices) within a single physical network, allowing different types of services or applications to share the same underlying infrastructure while maintaining their own unique characteristics and performance requirements.\\n\\nIn traditional networking, each service or application is typically run on its own separate network, which can lead to inefficiencies and cost overruns. Network slicing solves this problem by creating a virtual network for each slice, using a combination of software-defined networking (SDN), network functions virtualization (NFV), and edge computing.\\n\\nEach slice is designed to meet the specific needs of a particular service or application, such as low-latency video streaming or mission-critical IoT connectivity. By creating multiple slices, network operators can optimize their networks for different use cases, reduce costs, and improve overall efficiency.\\n\\nSome of the benefits of 5G Network Slicing include:\\n\\n* Improved performance and reliability for each slice\\n* Increased flexibility and scalability\\n* Reduced costs and energy consumption\\n* Enhanced security and isolation between slices\\n* Ability to create custom networks that meet specific service or application requirements\\n\\nOverall, 5G Network Slicing is an important technology that enables the creation of highly efficient, flexible, and customized networks that can support a wide range of use cases and applications.'}}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Örnek çalıştırma\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is 5G Network slicing?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m rag_graph\u001b[38;5;241m.\u001b[39mstream(test_input):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1724\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1724\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1725\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1726\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1727\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1728\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1729\u001b[0m         ):\n\u001b[1;32m   1730\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m     run_with_retry(\n\u001b[1;32m    231\u001b[0m         t,\n\u001b[1;32m    232\u001b[0m         retry_policy,\n\u001b[1;32m    233\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    234\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    235\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    236\u001b[0m         },\n\u001b[1;32m    237\u001b[0m     )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:495\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    492\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/langgraph/utils/runnable.py:259\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 259\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[3], line 87\u001b[0m, in \u001b[0;36mevaluate_response_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     81\u001b[0m evaluation_result \u001b[38;5;241m=\u001b[39m grader\u001b[38;5;241m.\u001b[39mgrade(\n\u001b[1;32m     82\u001b[0m     context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     83\u001b[0m     question\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_query\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     84\u001b[0m     response\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     86\u001b[0m score_lines \u001b[38;5;241m=\u001b[39m evaluation_result\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 87\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(score_lines[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# İlk satırdan skoru al\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: score}\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\u001b[0mDuring task with name 'evaluate_response' and id '29d18049-a544-c41f-8b8e-7ee7165b8f0a'"
     ]
    }
   ],
   "source": [
    "# Örnek çalıştırma\n",
    "test_input = {\"user_query\": \"What is 5G Network slicing?\"}\n",
    "for s in rag_graph.stream(test_input):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7a6fa-4c3f-4ca8-8e86-e5adfb537206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
