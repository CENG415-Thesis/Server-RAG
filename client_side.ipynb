{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20d0145-7618-4738-a943-a9e24ede12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://0105d35491599b8589.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0105d35491599b8589.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Mevcut vektör veritabanının adı\n",
    "DB_NAME = \"vector-db\"\n",
    "\n",
    "# Vektör veritabanını yükleme\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Ollama modelini başlat\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Chat Memory (Konuşma geçmişi için)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "def query_rag_pipeline(user_query, history):\n",
    "    # Konuşma geçmişini al\n",
    "    chat_history = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    # RAG ile en alakalı belgeleri al\n",
    "    retrieved_docs = vectorstore.similarity_search(user_query, k=10)\n",
    "    combined_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Prompt formatı\n",
    "    prompt = f\"\"\"\n",
    "    You are a telecom assistant. Your answers should be based on the context and chat history provided. If the context is not relevant to the user's query, politely state that you do not have the required information.\n",
    "\n",
    "    Chat History: {chat_history}\n",
    "    \n",
    "    Context: {combined_context}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Konuşma geçmişini güncelle\n",
    "    memory.save_context({\"input\": user_query}, {\"output\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "gr.ChatInterface(fn=query_rag_pipeline, title=\"Chatbot RAG Assistant 🤖\", type=\"messages\").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b8621-14ac-4977-a8d3-f8d2040db9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
