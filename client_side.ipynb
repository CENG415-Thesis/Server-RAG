{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20d0145-7618-4738-a943-a9e24ede12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "Running on public URL: https://0105d35491599b8589.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0105d35491599b8589.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Mevcut vekt繹r veritaban覺n覺n ad覺\n",
    "DB_NAME = \"vector-db\"\n",
    "\n",
    "# Vekt繹r veritaban覺n覺 y羹kleme\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Ollama modelini balat\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Chat Memory (Konuma ge癟mii i癟in)\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "def query_rag_pipeline(user_query, history):\n",
    "    # Konuma ge癟miini al\n",
    "    chat_history = memory.load_memory_variables({}).get(\"history\", \"\")\n",
    "    \n",
    "    # RAG ile en alakal覺 belgeleri al\n",
    "    retrieved_docs = vectorstore.similarity_search(user_query, k=10)\n",
    "    combined_context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # Prompt format覺\n",
    "    prompt = f\"\"\"\n",
    "    You are a telecom assistant. Your answers should be based on the context and chat history provided. If the context is not relevant to the user's query, politely state that you do not have the required information.\n",
    "\n",
    "    Chat History: {chat_history}\n",
    "    \n",
    "    Context: {combined_context}\n",
    "    \n",
    "    Question: {user_query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Konuma ge癟miini g羹ncelle\n",
    "    memory.save_context({\"input\": user_query}, {\"output\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Gradio UI\n",
    "gr.ChatInterface(fn=query_rag_pipeline, title=\"Chatbot RAG Assistant \", type=\"messages\").launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b8621-14ac-4977-a8d3-f8d2040db9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChainEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
